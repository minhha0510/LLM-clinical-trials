**\*\*EVALUATION TASK\*\***

**You will be provided with:**

**1.  A \*\*Candidate Prompt\*\*: The prompt to be evaluated (the "System" and "User" instructions for an LLM).**

**2.  \*\*Example LLM Output\*\*: The JSON output generated by the Candidate Prompt for one or more clinical trial records.**

**3.  \*\*The Project's Fixed Taxonomy\*\*: The list of allowed categories.**



**Your goal is to evaluate the \*\*Candidate Prompt\*\* by analyzing its design and the quality of the \*\*Example Output\*\* it produces. Use the detailed rubric below.**



**\*\*EVALUATION RUBRIC \& SCORING (Score each criterion 0-4)\*\***

**\*   \*\*4=Excellent, 3=Good, 2=Adequate, 1=Poor, 0=Failure/Critical Flaw\***



**\*\*1. SCALABILITY (Operational Efficiency at Scale)\*\***

**\*   \*\*Key Questions:\*\* Can this prompt run efficiently on 10,000+ trials? Does it avoid unnecessary complexity that slows batch processing?**

**\*   \*\*Checklist:\*\***

    **\*   `\[ ]` \*\*Deterministic Output:\*\* Prompt uses low `temperature` instruction and clear rules to minimize output variance across identical runs.**

    **\*   `\[ ]` \*\*Structured Output Mandate:\*\* Prompt explicitly demands a strict, parseable format (e.g., JSON). This is non-negotiable for automation.**

    **\*   `\[ ]` \*\*Context Length Management:\*\* Prompt includes strategy for long `detailed\_description` (e.g., instruction to "focus on first/last X words" or "extract only sentences mentioning termination").**

    **\*   `\[ ]` \*\*Self-Contained Logic:\*\* All necessary instructions, taxonomy, and examples are within the prompt. It does not rely on unstable chaining of multiple external calls.**



**\*\*2. ACCURACY (Faithfulness to Source \& Correct Categorization)\*\***

**\*   \*\*Key Questions:\*\* Does the output correctly reflect the source text? Are taxonomy categories applied correctly based on medical/logical reasoning?**

**\*   \*\*Checklist:\*\***

    **\*   `\[ ]` \*\*Evidence Grounding:\*\* The prompt forces the LLM to cite specific text snippets (quotes) as evidence for each decision.**

    **\*   `\[ ]` \*\*Taxonomy Fidelity:\*\* The prompt lists the taxonomy clearly and instructs the LLM to use \*only\* these categories, reducing hallucination of new labels.**

    **\*   `\[ ]` \*\*Multi-Label Handling:\*\* The prompt correctly instructs on handling trials with multiple primary reasons (e.g., not forcing a single choice).**

    **\*   `\[ ]` \*\*Ambiguity Handling:\*\* The prompt has a clear pathway for `UNKNOWN\_INSUFFICIENT\_INFO` when evidence is truly vague, preventing forced, incorrect guesses.**



**\*\*3. INTERPRETABILITY (Auditability \& Actionable Reasoning)\*\***

**\*   \*\*Key Questions:\*\* Can a human expert easily trace \*how\* the LLM arrived at its conclusion? Is the reasoning useful for downstream validation or error analysis?**

**\*   \*\*Checklist:\*\***

    **\*   `\[ ]` \*\*Explicit Reasoning Traces:\*\* The output includes a dedicated field (`reasoning\_traces`) that breaks down the logic per category.**

    **\*   `\[ ]` \*\*Evidence-Explanation Link:\*\* The reasoning clearly connects quoted evidence to the final category label (e.g., "`Inference:`" statement).**

    **\*   `\[ ]` \*\*Confidence Scoring:\*\* The prompt requires a `confidence` score, forcing the LLM to self-assess certainty, which flags cases for human review.**

    **\*   `\[ ]` \*\*Human-Readable Summary:\*\* The output includes a plain-language `explanation` field for quick scanning by non-technical stakeholders.**



**\*\*4. COST OPTIMIZATION (Token Efficiency vs. Performance)\*\***

**\*   \*\*Key Questions:\*\* Is the prompt designed to minimize token consumption (the primary cost driver) without sacrificing core accuracy?**

**\*   \*\*Checklist:\*\***

    **\*   `\[ ]` \*\*Concise In-Context Examples:\*\* Examples are minimal but sufficient, avoiding verbose, novel-length trial descriptions.**

    **\*   `\[ ]` \*\*Efficient Instruction Formatting:\*\* Instructions use clear bullet points or markdown without redundant or flowery language.**

    **\*   `\[ ]` \*\*Context Pruning:\*\* The prompt instructs the LLM to ignore irrelevant sections or summarizes lengthy inputs before analysis.**

    **\*   `\[ ]` \*\*Balanced Output:\*\* The required JSON structure includes all necessary fields but avoids extraneous, costly-to-generate commentary.**



**\*\*YOUR OUTPUT FORMAT\*\***

**Provide your evaluation in the following JSON structure. Be specific and reference parts of the Candidate Prompt and Example Output in your justifications.**



**{**

  **"evaluation\_summary": "A two-sentence overall impression of the prompt's readiness for production.",**

  **"scores": {**

    **"scalability": {"score": 0-4, "justification": "Reference specific prompt lines/strategies."},**

    **"accuracy": {"score": 0-4, "justification": "Reference evidence handling and taxonomy use."},**

    **"interpretability": {"score": 0-4, "justification": "Reference reasoning trace structure."},**

    **"cost": {"score": 0-4, "justification": "Analyze token efficiency of examples and instructions."}**

  **},**

  **"critical\_risks": \[**

    **"List any single points of failure (e.g., 'If the LLM ignores the JSON format instruction, the entire pipeline breaks.')"**

  **],**

  **"improvement\_recommendations": \[**

    **"Provide 2-3 concrete, actionable suggestions for prompt revision. (e.g., 'Add explicit instruction: If `why\_stopped` is only \\"Terminated,\\` default to UNKNOWN\_INSUFFICIENT\_INFO unless other fields override.\\`')"**

  **],**

  **"final\_verdict": "APPROVE | REVISE | REJECT. (REVISE if any score is 2 or below.)"**

**}**

